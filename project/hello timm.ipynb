{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-31T05:39:53.984994Z",
     "start_time": "2025-10-31T05:39:49.826759Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from my_utils import find_files_by_ext"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(timm.__version__)  # 1.0.21\n",
    "print(len(timm.list_models()))  # 1279\n",
    "print(len(timm.list_models(pretrained=True)))  # 1689\n",
    "print(len(timm.list_models('*efficientnetv2*', pretrained=True)))  # 21\n",
    "print(timm.list_models('*efficientnetv2*', pretrained=True))"
   ],
   "id": "62505c2456a1cead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "timm.list_models('*convnext*', pretrained=True)",
   "id": "1c1e83ee4cda838b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model = timm.create_model('tf_efficientnetv2_s.in1k',\n",
    "#                           pretrained=True,\n",
    "#                           cache_dir=r\"E:\\Git\\pytorch-image-models\\models\")\n",
    "# model = timm.create_model('resnet50d', pretrained=True,\n",
    "#                           num_classes=10,\n",
    "#                           cache_dir=r\"E:\\Git\\pytorch-image-models\\models\")\n",
    "# print(model)\n",
    "# print(model.get_classifier())  # Linear(in_features=2048, out_features=10, bias=True)\n",
    "# print(model.global_pool)  # SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
    "#\n",
    "# pool_types = ['avg', 'max', 'avgmax', 'catavgmax', '']\n",
    "#\n",
    "# for pool in pool_types:\n",
    "#     model = timm.create_model('resnet50d', pretrained=True,\n",
    "#                               num_classes=0, global_pool=pool,\n",
    "#                               cache_dir=r\"E:\\Git\\pytorch-image-models\\models\")\n",
    "#     model.eval()\n",
    "#     feature_output = model(torch.randn(1, 3, 224, 224))\n",
    "#     print(feature_output.shape)\n",
    "\n",
    "model = timm.create_model('resnet50d', pretrained=True,\n",
    "                          num_classes=10, global_pool='catavgmax',\n",
    "                          cache_dir=r\"E:\\Git\\pytorch-image-models\\models\")\n",
    "print(model.get_classifier())\n",
    "num_in_features = model.get_classifier().in_features\n",
    "print(num_in_features)\n",
    "# 修改分类头\n",
    "model.fc = nn.Sequential(\n",
    "    nn.BatchNorm1d(num_in_features),\n",
    "    nn.Linear(in_features=num_in_features, out_features=512, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(in_features=512, out_features=10, bias=False)\n",
    ")\n",
    "print(model.get_classifier())\n",
    "model.eval()\n",
    "output = model(torch.randn(1, 3, 224, 224))\n",
    "print(output.shape)"
   ],
   "id": "7bf489410aef9503",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = timm.create_model('resnet50d', pretrained=True, num_classes=10, cache_dir=r\"E:\\Git\\pytorch-image-models\\models\")\n",
    "model"
   ],
   "id": "47413fdc3df576d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = timm.create_model('resnet50d', pretrained=True, num_classes=10, features_only=True, cache_dir=r\"E:\\Git\\pytorch-image-models\\models\")\n",
    "model"
   ],
   "id": "54916205c4b57fff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T05:40:06.987113Z",
     "start_time": "2025-10-31T05:39:56.555893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = timm.create_model(\n",
    "            \"convnextv2_tiny.fcmae_ft_in1k\",\n",
    "            pretrained=True,\n",
    "            num_classes=2,  # 类别数\n",
    "            # features_only=True,  # 输入通道数\n",
    "            cache_dir=r\"E:\\Git\\pytorch-image-models\\models\"  # 权重缓存路径（可选）\n",
    "        )"
   ],
   "id": "3436885920c8650",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    if 'head' not in name:\n",
    "        param.requires_grad = False"
   ],
   "id": "225fbb6a4fcd9989",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T05:39:21.149098Z",
     "start_time": "2025-10-31T05:39:21.132100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 记录可训练参数\n",
    "trainable_params = 0\n",
    "frozen_params = 0\n",
    "freeze_backbone = True\n",
    "freeze_layers = None\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    # 默认不冻结\n",
    "    requires_grad = True\n",
    "\n",
    "    # 如果指定了冻结层列表\n",
    "    if freeze_layers is not None and any(layer in name for layer in freeze_layers):\n",
    "        requires_grad = False\n",
    "\n",
    "    # 如果全局冻结骨干网络（且未被冻结层列表覆盖）\n",
    "    elif freeze_backbone and not any(head_key in name for head_key in ['head', 'classifier', 'pred']):\n",
    "        requires_grad = False\n",
    "\n",
    "    param.requires_grad = requires_grad\n",
    "    # 统计参数数量\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "    else:\n",
    "        frozen_params += param.numel()\n",
    "\n",
    "print(f\"参数统计: 可训练参数 {trainable_params:,} | 冻结参数 {frozen_params:,} | 总参数 {trainable_params + frozen_params:,}\")"
   ],
   "id": "4d062d6ad571afe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stem.0.weight torch.Size([96, 3, 4, 4])\n",
      "stem.0.bias torch.Size([96])\n",
      "stem.1.weight torch.Size([96])\n",
      "stem.1.bias torch.Size([96])\n",
      "stages.0.blocks.0.conv_dw.weight torch.Size([96, 1, 7, 7])\n",
      "stages.0.blocks.0.conv_dw.bias torch.Size([96])\n",
      "stages.0.blocks.0.norm.weight torch.Size([96])\n",
      "stages.0.blocks.0.norm.bias torch.Size([96])\n",
      "stages.0.blocks.0.mlp.fc1.weight torch.Size([384, 96])\n",
      "stages.0.blocks.0.mlp.fc1.bias torch.Size([384])\n",
      "stages.0.blocks.0.mlp.grn.weight torch.Size([384])\n",
      "stages.0.blocks.0.mlp.grn.bias torch.Size([384])\n",
      "stages.0.blocks.0.mlp.fc2.weight torch.Size([96, 384])\n",
      "stages.0.blocks.0.mlp.fc2.bias torch.Size([96])\n",
      "stages.0.blocks.1.conv_dw.weight torch.Size([96, 1, 7, 7])\n",
      "stages.0.blocks.1.conv_dw.bias torch.Size([96])\n",
      "stages.0.blocks.1.norm.weight torch.Size([96])\n",
      "stages.0.blocks.1.norm.bias torch.Size([96])\n",
      "stages.0.blocks.1.mlp.fc1.weight torch.Size([384, 96])\n",
      "stages.0.blocks.1.mlp.fc1.bias torch.Size([384])\n",
      "stages.0.blocks.1.mlp.grn.weight torch.Size([384])\n",
      "stages.0.blocks.1.mlp.grn.bias torch.Size([384])\n",
      "stages.0.blocks.1.mlp.fc2.weight torch.Size([96, 384])\n",
      "stages.0.blocks.1.mlp.fc2.bias torch.Size([96])\n",
      "stages.0.blocks.2.conv_dw.weight torch.Size([96, 1, 7, 7])\n",
      "stages.0.blocks.2.conv_dw.bias torch.Size([96])\n",
      "stages.0.blocks.2.norm.weight torch.Size([96])\n",
      "stages.0.blocks.2.norm.bias torch.Size([96])\n",
      "stages.0.blocks.2.mlp.fc1.weight torch.Size([384, 96])\n",
      "stages.0.blocks.2.mlp.fc1.bias torch.Size([384])\n",
      "stages.0.blocks.2.mlp.grn.weight torch.Size([384])\n",
      "stages.0.blocks.2.mlp.grn.bias torch.Size([384])\n",
      "stages.0.blocks.2.mlp.fc2.weight torch.Size([96, 384])\n",
      "stages.0.blocks.2.mlp.fc2.bias torch.Size([96])\n",
      "stages.1.downsample.0.weight torch.Size([96])\n",
      "stages.1.downsample.0.bias torch.Size([96])\n",
      "stages.1.downsample.1.weight torch.Size([192, 96, 2, 2])\n",
      "stages.1.downsample.1.bias torch.Size([192])\n",
      "stages.1.blocks.0.conv_dw.weight torch.Size([192, 1, 7, 7])\n",
      "stages.1.blocks.0.conv_dw.bias torch.Size([192])\n",
      "stages.1.blocks.0.norm.weight torch.Size([192])\n",
      "stages.1.blocks.0.norm.bias torch.Size([192])\n",
      "stages.1.blocks.0.mlp.fc1.weight torch.Size([768, 192])\n",
      "stages.1.blocks.0.mlp.fc1.bias torch.Size([768])\n",
      "stages.1.blocks.0.mlp.grn.weight torch.Size([768])\n",
      "stages.1.blocks.0.mlp.grn.bias torch.Size([768])\n",
      "stages.1.blocks.0.mlp.fc2.weight torch.Size([192, 768])\n",
      "stages.1.blocks.0.mlp.fc2.bias torch.Size([192])\n",
      "stages.1.blocks.1.conv_dw.weight torch.Size([192, 1, 7, 7])\n",
      "stages.1.blocks.1.conv_dw.bias torch.Size([192])\n",
      "stages.1.blocks.1.norm.weight torch.Size([192])\n",
      "stages.1.blocks.1.norm.bias torch.Size([192])\n",
      "stages.1.blocks.1.mlp.fc1.weight torch.Size([768, 192])\n",
      "stages.1.blocks.1.mlp.fc1.bias torch.Size([768])\n",
      "stages.1.blocks.1.mlp.grn.weight torch.Size([768])\n",
      "stages.1.blocks.1.mlp.grn.bias torch.Size([768])\n",
      "stages.1.blocks.1.mlp.fc2.weight torch.Size([192, 768])\n",
      "stages.1.blocks.1.mlp.fc2.bias torch.Size([192])\n",
      "stages.1.blocks.2.conv_dw.weight torch.Size([192, 1, 7, 7])\n",
      "stages.1.blocks.2.conv_dw.bias torch.Size([192])\n",
      "stages.1.blocks.2.norm.weight torch.Size([192])\n",
      "stages.1.blocks.2.norm.bias torch.Size([192])\n",
      "stages.1.blocks.2.mlp.fc1.weight torch.Size([768, 192])\n",
      "stages.1.blocks.2.mlp.fc1.bias torch.Size([768])\n",
      "stages.1.blocks.2.mlp.grn.weight torch.Size([768])\n",
      "stages.1.blocks.2.mlp.grn.bias torch.Size([768])\n",
      "stages.1.blocks.2.mlp.fc2.weight torch.Size([192, 768])\n",
      "stages.1.blocks.2.mlp.fc2.bias torch.Size([192])\n",
      "stages.2.downsample.0.weight torch.Size([192])\n",
      "stages.2.downsample.0.bias torch.Size([192])\n",
      "stages.2.downsample.1.weight torch.Size([384, 192, 2, 2])\n",
      "stages.2.downsample.1.bias torch.Size([384])\n",
      "stages.2.blocks.0.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.0.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.0.norm.weight torch.Size([384])\n",
      "stages.2.blocks.0.norm.bias torch.Size([384])\n",
      "stages.2.blocks.0.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.0.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.0.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.0.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.0.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.0.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.1.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.1.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.1.norm.weight torch.Size([384])\n",
      "stages.2.blocks.1.norm.bias torch.Size([384])\n",
      "stages.2.blocks.1.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.1.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.1.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.1.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.1.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.1.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.2.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.2.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.2.norm.weight torch.Size([384])\n",
      "stages.2.blocks.2.norm.bias torch.Size([384])\n",
      "stages.2.blocks.2.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.2.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.2.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.2.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.2.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.2.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.3.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.3.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.3.norm.weight torch.Size([384])\n",
      "stages.2.blocks.3.norm.bias torch.Size([384])\n",
      "stages.2.blocks.3.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.3.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.3.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.3.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.3.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.3.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.4.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.4.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.4.norm.weight torch.Size([384])\n",
      "stages.2.blocks.4.norm.bias torch.Size([384])\n",
      "stages.2.blocks.4.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.4.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.4.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.4.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.4.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.4.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.5.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.5.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.5.norm.weight torch.Size([384])\n",
      "stages.2.blocks.5.norm.bias torch.Size([384])\n",
      "stages.2.blocks.5.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.5.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.5.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.5.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.5.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.5.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.6.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.6.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.6.norm.weight torch.Size([384])\n",
      "stages.2.blocks.6.norm.bias torch.Size([384])\n",
      "stages.2.blocks.6.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.6.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.6.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.6.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.6.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.6.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.7.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.7.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.7.norm.weight torch.Size([384])\n",
      "stages.2.blocks.7.norm.bias torch.Size([384])\n",
      "stages.2.blocks.7.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.7.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.7.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.7.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.7.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.7.mlp.fc2.bias torch.Size([384])\n",
      "stages.2.blocks.8.conv_dw.weight torch.Size([384, 1, 7, 7])\n",
      "stages.2.blocks.8.conv_dw.bias torch.Size([384])\n",
      "stages.2.blocks.8.norm.weight torch.Size([384])\n",
      "stages.2.blocks.8.norm.bias torch.Size([384])\n",
      "stages.2.blocks.8.mlp.fc1.weight torch.Size([1536, 384])\n",
      "stages.2.blocks.8.mlp.fc1.bias torch.Size([1536])\n",
      "stages.2.blocks.8.mlp.grn.weight torch.Size([1536])\n",
      "stages.2.blocks.8.mlp.grn.bias torch.Size([1536])\n",
      "stages.2.blocks.8.mlp.fc2.weight torch.Size([384, 1536])\n",
      "stages.2.blocks.8.mlp.fc2.bias torch.Size([384])\n",
      "stages.3.downsample.0.weight torch.Size([384])\n",
      "stages.3.downsample.0.bias torch.Size([384])\n",
      "stages.3.downsample.1.weight torch.Size([768, 384, 2, 2])\n",
      "stages.3.downsample.1.bias torch.Size([768])\n",
      "stages.3.blocks.0.conv_dw.weight torch.Size([768, 1, 7, 7])\n",
      "stages.3.blocks.0.conv_dw.bias torch.Size([768])\n",
      "stages.3.blocks.0.norm.weight torch.Size([768])\n",
      "stages.3.blocks.0.norm.bias torch.Size([768])\n",
      "stages.3.blocks.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "stages.3.blocks.0.mlp.fc1.bias torch.Size([3072])\n",
      "stages.3.blocks.0.mlp.grn.weight torch.Size([3072])\n",
      "stages.3.blocks.0.mlp.grn.bias torch.Size([3072])\n",
      "stages.3.blocks.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "stages.3.blocks.0.mlp.fc2.bias torch.Size([768])\n",
      "stages.3.blocks.1.conv_dw.weight torch.Size([768, 1, 7, 7])\n",
      "stages.3.blocks.1.conv_dw.bias torch.Size([768])\n",
      "stages.3.blocks.1.norm.weight torch.Size([768])\n",
      "stages.3.blocks.1.norm.bias torch.Size([768])\n",
      "stages.3.blocks.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "stages.3.blocks.1.mlp.fc1.bias torch.Size([3072])\n",
      "stages.3.blocks.1.mlp.grn.weight torch.Size([3072])\n",
      "stages.3.blocks.1.mlp.grn.bias torch.Size([3072])\n",
      "stages.3.blocks.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "stages.3.blocks.1.mlp.fc2.bias torch.Size([768])\n",
      "stages.3.blocks.2.conv_dw.weight torch.Size([768, 1, 7, 7])\n",
      "stages.3.blocks.2.conv_dw.bias torch.Size([768])\n",
      "stages.3.blocks.2.norm.weight torch.Size([768])\n",
      "stages.3.blocks.2.norm.bias torch.Size([768])\n",
      "stages.3.blocks.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "stages.3.blocks.2.mlp.fc1.bias torch.Size([3072])\n",
      "stages.3.blocks.2.mlp.grn.weight torch.Size([3072])\n",
      "stages.3.blocks.2.mlp.grn.bias torch.Size([3072])\n",
      "stages.3.blocks.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "stages.3.blocks.2.mlp.fc2.bias torch.Size([768])\n",
      "head.norm.weight torch.Size([768])\n",
      "head.norm.bias torch.Size([768])\n",
      "head.fc.weight torch.Size([2, 768])\n",
      "head.fc.bias torch.Size([2])\n",
      "参数统计: 可训练参数 3,074 | 冻结参数 27,864,960 | 总参数 27,868,034\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T05:43:41.700327Z",
     "start_time": "2025-10-31T05:43:41.680684Z"
    }
   },
   "cell_type": "code",
   "source": "model.head",
   "id": "753b8ed70e879787",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormMlpClassifierHead(\n",
       "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "  (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (pre_logits): Identity()\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T05:44:17.740973Z",
     "start_time": "2025-10-31T05:44:17.736969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainable_params = 0\n",
    "frozen_params = 0\n",
    "for name, param in model.head.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    # 统计参数数量\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "    else:\n",
    "        frozen_params += param.numel()\n",
    "\n",
    "print(f\"参数统计: 可训练参数 {trainable_params:,} | 冻结参数 {frozen_params:,} | 总参数 {trainable_params + frozen_params:,}\")"
   ],
   "id": "ff037732e16b3e5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm.weight torch.Size([768])\n",
      "norm.bias torch.Size([768])\n",
      "fc.weight torch.Size([2, 768])\n",
      "fc.bias torch.Size([2])\n",
      "参数统计: 可训练参数 3,074 | 冻结参数 0 | 总参数 3,074\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### data",
   "id": "4ef60f9f542fa951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from timm.data.transforms_factory import create_transform\n",
    "# print(create_transform(224,))\n",
    "# print(create_transform(224, is_training=True))\n",
    "# create_transform(224, is_training=True, auto_augment='rand-m9-mstd0.5')\n",
    "from timm.data.auto_augment import rand_augment_transform\n",
    "tfm = rand_augment_transform(config_str='rand-m9-mstd0.5', hparams={'img_mean': (124, 116, 104)})\n",
    "print(tfm)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(r\"E:\\Data\\TrainSet\\13_HS_CaF2_cls\\1029_a1b8\\images\\train\\1\\3_44.bmp\")\n",
    "\n",
    "# from timm.data.transforms import RandomResizedCropAndInterpolation\n",
    "#\n",
    "# tfm = RandomResizedCropAndInterpolation(size=224, interpolation='random')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(10, 5))\n",
    "\n",
    "for i in range(2):\n",
    "    for idx, im in enumerate([tfm(img) for i in range(4)]):\n",
    "        ax[i, idx].imshow(im)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9888f92e117625ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### dataset",
   "id": "dffc30f76c8f1952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from timm.data import ImageDataset, create_transform\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader_iterator():\n",
    "    dataset = ImageDataset(r'E:\\Data\\TrainSet\\01_Ore_seg\\images', transform=create_transform(224))\n",
    "    dl = iter(DataLoader(dataset, batch_size=4))\n",
    "    return dl"
   ],
   "id": "9f4c229b3a005d04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataloader = create_dataloader_iterator()",
   "id": "1ae37e8e9dfb29a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "inputs, classes = next(dataloader)",
   "id": "b0eb7c6c3a4f2693",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import numpy as np"
   ],
   "id": "52e4ad8a8a75701d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = np.clip(std * inp + mean, 0,1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ],
   "id": "f8a33e055754e168",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "out = torchvision.utils.make_grid(inputs)\n",
    "imshow(out, title=[x.item() for x in classes])"
   ],
   "id": "983af847ed42481b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
